{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAt-K2qgcIou"
   },
   "source": [
    "# Optimization Using Gradient Descent: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZYK-0rin5x7"
   },
   "source": [
    "In this assignment, you will build a simple linear regression model to predict sales based on TV marketing expenses. You will investigate three different approaches to this problem. You will use `NumPy` and `Scikit-Learn` linear regression models, as well as construct and optimize the sum of squares cost function with gradient descent from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [ 1 - Open the Dataset and State the Problem](#1)\n",
    "  - [ Exercise 1](#ex01)\n",
    "- [ 2 - Linear Regression in Python with `NumPy` and `Scikit-Learn`](#2)\n",
    "  - [ 2.1 - Linear Regression with `NumPy`](#2.1)\n",
    "    - [ Exercise 2](#ex02)\n",
    "  - [ 2.2 - Linear Regression with `Scikit-Learn`](#2.2)\n",
    "    - [ Exercise 3](#ex03)\n",
    "    - [ Exercise 4](#ex04)\n",
    "- [ 3 - Linear Regression using Gradient Descent](#3)\n",
    "  - [ Exercise 5](#ex05)\n",
    "  - [ Exercise 6](#ex06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "Load the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A library for programmatic plot generation.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# A library for data manipulation and analysis.\n",
    "import pandas as pd\n",
    "\n",
    "# LinearRegression from sklearn.\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the unit tests defined for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import w2_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Open the Dataset and State the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will build a linear regression model for a simple [Kaggle dataset](https://www.kaggle.com/code/devzohaib/simple-linear-regression/notebook), saved in a file `data/tvmarketing.csv`. The dataset has only two fields: TV marketing expenses (`TV`) and sales amount (`Sales`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex01'></a>\n",
    "### Exercise 1\n",
    "\n",
    "Use `pandas` function `pd.read_csv` to open the .csv file the from the `path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "path = \"./tvmarketing.csv\"\n",
    "\n",
    "### START CODE HERE ### (~ 1 line of code)\n",
    "adv = pd.read_csv(path)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  Sales\n",
       "0  230.1   22.1\n",
       "1   44.5   10.4\n",
       "2   17.2    9.3\n",
       "3  151.5   18.5\n",
       "4  180.8   12.9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print some part of the dataset.\n",
    "adv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "\n",
    "```Python\n",
    "\tTV\tSales\n",
    "0\t230.1\t22.1\n",
    "1\t44.5\t10.4\n",
    "2\t17.2\t9.3\n",
    "3\t151.5\t18.5\n",
    "4\t180.8\t12.9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2_unittest.test_load_data(adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` has a function to make plots from the DataFrame fields. By default, matplotlib is used at the backend. Let's use it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='TV', ylabel='Sales'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adv.plot(x=\"TV\", y=\"Sales\", kind=\"scatter\", c=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this dataset to solve a simple problem with linear regression: given a TV marketing budget, predict sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Linear Regression in Python with `NumPy` and `Scikit-Learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the required field of the DataFrame into variables `X` and `Y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "X = adv[\"TV\"]\n",
    "Y = adv[\"Sales\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 - Linear Regression with `NumPy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the function `np.polyfit(x, y, deg)` to fit a polynomial of degree `deg` to points $(x, y)$, minimising the sum of squared errors. You can read more in the [documentation](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html). Taking `deg = 1` you can obtain the slope `m` and the intercept `b` of the linear regression line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression with NumPy. Slope: 0.04753664043301972. Intercept: 7.032593549127696\n"
     ]
    }
   ],
   "source": [
    "m_numpy, b_numpy = np.polyfit(X, Y, 1)\n",
    "\n",
    "print(f\"Linear regression with NumPy. Slope: {m_numpy}. Intercept: {b_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: [`NumPy` documentation](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) suggests the [`Polynomial.fit` class method](https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html#numpy.polynomial.polynomial.Polynomial.fit) as recommended for new code as it is more stable numerically. But in this simple example, you can stick to the `np.polyfit` function for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot the linear regression line by running the following code. The regression line is red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linear_regression(\n",
    "    X, Y, x_label, y_label, m, b, X_pred=np.array([]), Y_pred=np.array([])\n",
    "):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "    ax.plot(X, Y, \"o\", color=\"black\")\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "\n",
    "\n",
    "    ax.plot(X, m * X + b, color=\"red\")\n",
    "\n",
    "    # Plot prediction points (empty arrays by default - the predictions will be calculated later).\n",
    "\n",
    "    ax.plot(X_pred, Y_pred, \"o\", color=\"blue\", markersize=8)\n",
    "\n",
    "\n",
    "\n",
    "plot_linear_regression(X, Y, \"TV\", \"Sales\", m_numpy, b_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex02'></a>\n",
    "### Exercise 2\n",
    "\n",
    "Make predictions substituting the obtained slope and intercept coefficients into the equation $Y = mX + b$, given an array of $X$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# This is organised as a function only for grading purposes.\n",
    "def pred_numpy(m, b, X):\n",
    "    ### START CODE HERE ### (~ 1 line of code)\n",
    "    Y = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "X_pred = np.array([50, 120, 280])\n",
    "Y_pred_numpy = pred_numpy(m_numpy, b_numpy, X_pred)\n",
    "\n",
    "print(f\"TV marketing expenses:\\n{X_pred}\")\n",
    "print(f\"Predictions of sales using NumPy linear regression:\\n{Y_pred_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "\n",
    "```Python\n",
    "TV marketing expenses:\n",
    "[ 50 120 280]\n",
    "Predictions of sales using NumPy linear regression:\n",
    "[ 9.40942557 12.7369904  20.34285287]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_unittest.test_pred_numpy(pred_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can add the prediction points to the plot (blue dots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear_regression(X, Y, \"TV\", \"Sales\", m_numpy, b_numpy, X_pred, Y_pred_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 - Linear Regression with `Scikit-Learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Scikit-Learn` is an open-source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities. `Scikit-learn` provides dozens of built-in machine learning algorithms and models, called **estimators**. Each estimator can be fitted to some data using its `fit` method. Full documentation can be found [here](https://scikit-learn.org/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an estimator object for a linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "lr_sklearn = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimator can learn from data calling the `fit` function. However, trying to run the following code you will get an error, as the data needs to be reshaped into 2D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "print(f\"Shape of X array: {X.shape}\")\n",
    "print(f\"Shape of Y array: {Y.shape}\")\n",
    "\n",
    "try:\n",
    "    lr_sklearn.fit(X, Y)\n",
    "except ValueError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can increase the dimension of the array by one with `reshape` function, or there is another another way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "X_sklearn = X[:, np.newaxis]\n",
    "Y_sklearn = Y[:, np.newaxis]\n",
    "\n",
    "print(f\"Shape of new X array: {X_sklearn.shape}\")\n",
    "print(f\"Shape of new Y array: {Y_sklearn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex03'></a>\n",
    "### Exercise 3\n",
    "\n",
    "Fit the linear regression model passing `X_sklearn` and `Y_sklearn` arrays into the function `lr_sklearn.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (~ 1 line of code)\n",
    "None.None(None, None)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "m_sklearn = lr_sklearn.coef_\n",
    "b_sklearn = lr_sklearn.intercept_\n",
    "\n",
    "print(\n",
    "    f\"Linear regression using Scikit-Learn. Slope: {m_sklearn}. Intercept: {b_sklearn}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "\n",
    "```Python\n",
    "Linear regression using Scikit-Learn. Slope: [[0.04753664]]. Intercept: [7.03259355]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_unittest.test_sklearn_fit(lr_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you have got the same result as with the `NumPy` function `polyfit`. Now, to make predictions it is convenient to use `Scikit-Learn` function `predict`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex04'></a>\n",
    "### Exercise 4\n",
    "\n",
    "\n",
    "Increase the dimension of the $X$ array using the function `np.newaxis` (see an example above) and pass the result to the `lr_sklearn.predict` function to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# This is organised as a function only for grading purposes.\n",
    "def pred_sklearn(X, lr_sklearn):\n",
    "    ### START CODE HERE ### (~ 2 lines of code)\n",
    "    X_2D = None[None, None.None]\n",
    "    Y = None.None(None)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "Y_pred_sklearn = pred_sklearn(X_pred, lr_sklearn)\n",
    "\n",
    "print(f\"TV marketing expenses:\\n{X_pred}\")\n",
    "print(f\"Predictions of sales using Scikit_Learn linear regression:\\n{Y_pred_sklearn.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "\n",
    "```Python\n",
    "TV marketing expenses:\n",
    "[ 50 120 280]\n",
    "Predictions of sales using Scikit_Learn linear regression:\n",
    "[[ 9.40942557 12.7369904  20.34285287]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_unittest.test_sklearn_predict(pred_sklearn, lr_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted values are also the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Linear Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to fit the models automatically are convenient to use, but for an in-depth understanding of the model and the maths behind it is good to implement an algorithm by yourself. Let's try to find linear regression coefficients $m$ and $b$, by minimising the difference between original values $y^{(i)}$ and predicted values $\\hat{y}^{(i)}$ with the **loss function** $L\\left(w, b\\right)  = \\frac{1}{2}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$ for each of the training examples. Division by $2$ is taken just for scaling purposes, you will see the reason below, calculating partial derivatives.\n",
    "\n",
    "To compare the resulting vector of the predictions $\\hat{Y}$ with the vector $Y$ of original values $y^{(i)}$, you can take an average of the loss function values for each of the training examples:\n",
    "\n",
    "$$E\\left(m, b\\right) = \\frac{1}{2n}\\sum_{i=1}^{n} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 = \n",
    "\\frac{1}{2n}\\sum_{i=1}^{n} \\left(mx^{(i)}+b - y^{(i)}\\right)^2,\\tag{1}$$\n",
    "\n",
    "where $n$ is a number of data points. This function is called the sum of squares **cost function**. To use gradient descent algorithm, calculate partial derivatives as:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E }{ \\partial m } &= \n",
    "\\frac{1}{n}\\sum_{i=1}^{n} \\left(mx^{(i)}+b - y^{(i)}\\right)x^{(i)},\\\\\n",
    "\\frac{\\partial E }{ \\partial b } &= \n",
    "\\frac{1}{n}\\sum_{i=1}^{n} \\left(mx^{(i)}+b - y^{(i)}\\right),\n",
    "\\tag{2}\\end{align}\n",
    "\n",
    "and update the parameters iteratively using the expressions\n",
    "\n",
    "\\begin{align}\n",
    "m &= m - \\alpha \\frac{\\partial E }{ \\partial m },\\\\\n",
    "b &= b - \\alpha \\frac{\\partial E }{ \\partial b },\n",
    "\\tag{3}\\end{align}\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original arrays `X` and `Y` have different units. To make gradient descent algorithm efficient, you need to bring them to the same units. A common approach to it is called **normalization**: substract the mean value of the array from each of the elements in the array and divide them by standard deviation (a statistical measure of the amount of dispersion of a set of values). If you are not familiar with mean and standard deviation, do not worry about this for now - this is covered in the next Course of Specialization.\n",
    "\n",
    "Normalization is not compulsory - gradient descent would work without it. But due to different units of `X` and `Y`, the cost function will be much steeper. Then you would need to take a significantly smaller learning rate $\\alpha$, and the algorithm will require thousands of iterations to converge instead of a few dozens. Normalization helps to increase the efficiency of the gradient descent algorithm.\n",
    "\n",
    "Normalization is implemented in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "X_norm = (X - np.mean(X)) / np.std(X)\n",
    "Y_norm = (Y - np.mean(Y)) / np.std(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define cost function according to the equation $(1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def E(m, b, X, Y):\n",
    "    return 1 / (2 * len(Y)) * np.sum((m * X + b - Y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex05'></a>\n",
    "### Exercise 5\n",
    "\n",
    "\n",
    "Define functions `dEdm` and `dEdb` to calculate partial derivatives according to the equations $(2)$. This can be done using vector form of the input data `X` and `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def dEdm(m, b, X, Y):\n",
    "    ### START CODE HERE ### (~ 1 line of code)\n",
    "    # Use the following line as a hint, replacing all None.\n",
    "    res = 1 / len(None) * np.dot(None * None + None - None, None)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def dEdb(m, b, X, Y):\n",
    "    ### START CODE HERE ### (~ 1 line of code)\n",
    "    # Replace None writing the required expression fully.\n",
    "    res = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "print(dEdm(0, 0, X_norm, Y_norm))\n",
    "print(dEdb(0, 0, X_norm, Y_norm))\n",
    "print(dEdm(1, 5, X_norm, Y_norm))\n",
    "print(dEdb(1, 5, X_norm, Y_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "\n",
    "```Python\n",
    "-0.7822244248616067\n",
    "5.098005351200641e-16\n",
    "0.21777557513839355\n",
    "5.000000000000002\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_unittest.test_partial_derivatives(dEdm, dEdb, X_norm, Y_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex06'></a>\n",
    "### Exercise 6\n",
    "\n",
    "\n",
    "Implement gradient descent using expressions $(3)$:\n",
    "\\begin{align}\n",
    "m &= m - \\alpha \\frac{\\partial E }{ \\partial m },\\\\\n",
    "b &= b - \\alpha \\frac{\\partial E }{ \\partial b },\n",
    "\\end{align}\n",
    "\n",
    "where $\\alpha$ is the `learning_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    dEdm, dEdb, m, b, X, Y, learning_rate=0.001, num_iterations=1000, print_cost=False\n",
    "):\n",
    "    for iteration in range(num_iterations):\n",
    "\n",
    "        ### START CODE HERE ### (~ 2 lines of code)\n",
    "\n",
    "        m_new = None\n",
    "\n",
    "        b_new = None\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        m = m_new\n",
    "\n",
    "        b = b_new\n",
    "\n",
    "        if print_cost:\n",
    "\n",
    "            print(f\"Cost after iteration {iteration}: {E(m, b, X, Y)}\")\n",
    "\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "print(gradient_descent(dEdm, dEdb, 0, 0, X_norm, Y_norm))\n",
    "print(\n",
    "    gradient_descent(\n",
    "        dEdm, dEdb, 1, 5, X_norm, Y_norm, learning_rate=0.01, num_iterations=10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "\n",
    "```Python\n",
    "(0.49460408269589495, -3.489285249624889e-16)\n",
    "(0.9791767513915026, 4.521910375044022)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_unittest.test_gradient_descent(gradient_descent, dEdm, dEdb, X_norm, Y_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the gradient descent method starting from the initial point $\\left(m_0, b_0\\right)=\\left(0, 0\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "m_initial = 0\n",
    "b_initial = 0\n",
    "num_iterations = 30\n",
    "learning_rate = 1.2\n",
    "\n",
    "\n",
    "m_gd, b_gd = gradient_descent(\n",
    "    dEdm,\n",
    "    dEdb,\n",
    "    m_initial,\n",
    "    b_initial,\n",
    "    X_norm,\n",
    "    Y_norm,\n",
    "    learning_rate,\n",
    "    num_iterations,\n",
    "    print_cost=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Gradient descent result: m_min, b_min = {m_gd}, {b_gd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, that the initial datasets were normalized. To make the predictions, you need to normalize `X_pred` array, calculate `Y_pred` with the linear regression coefficients `m_gd`, `b_gd` and then **denormalize** the result (perform the reverse process of normalization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "X_pred = np.array([50, 120, 280])\n",
    "# Use the same mean and standard deviation of the original training array X\n",
    "X_pred_norm = (X_pred - np.mean(X)) / np.std(X)\n",
    "Y_pred_gd_norm = m_gd * X_pred_norm + b_gd\n",
    "# Use the same mean and standard deviation of the original training array Y\n",
    "Y_pred_gd = Y_pred_gd_norm * np.std(Y) + np.mean(Y)\n",
    "\n",
    "print(f\"TV marketing expenses:\\n{X_pred}\")\n",
    "print(f\"Predictions of sales using Scikit_Learn linear regression:\\n{Y_pred_sklearn.T}\")\n",
    "print(f\"Predictions of sales using Gradient Descent:\\n{Y_pred_gd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have gotten similar results as in the previous sections. \n",
    "\n",
    "Well done! Now you know how gradient descent algorithm can be applied to train a real model. Re-producing results manually for a simple case should give you extra confidence that you understand what happends under the hood of commonly used functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C1_W1_Assignment_Solution.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "AI4MC1-1"
   ]
  },
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "478841ab876a4250505273c8a697bbc1b6b194054b009c227dc606f17fb56272"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
